---
title: "Conceptos Simulación Digital"
author: "Anderson Acuña"
date: "2024-04-02"
output: 
  html_document: 
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Números Pseudoaleatorios
Estos son los números que generamos por medio de algoritmos, y estos números simulan ser **aleatorios**.
Indiferentente de que sean generados por un algoritmo, estadísticamente deben cumplir dos criterios:

+ Uniformidad
+ Independencia

## Método Multiplicativo Congruencial
Se construyen a través de la siguiente ecuación:
$$
X_n = (a*X_{n-1})mod(n)
$$

## Generadores Congruenciales Lineales
$$
X_n = (aX_{n-1} + C)mod(m)
$$

```{r}
x_n = c(27)
const_a = 17
const_c = 43
const_m = 100

for(number in 1:20) {
  x_value = ((const_a * x_n[number] + const_c) %% const_m)
  x_n = c(x_n, x_value)
}

r_n = c(x_n / const_m)
```

```{r}
plot(x_n)
```

La saiguiente configuración fue un generador de los más utilizados en los 80's:
```{r}

x_n = c(42)
const_a = 75
const_c = 0
const_m = 2**31 - 1

for(number in 1:200) {
  x_value = ((const_a * x_n[number] + const_c) %% const_m)
  x_n = c(x_n, x_value)
}

r_n = c(x_n / const_m)
```

```{r}
plot(x_n)
```

## Pruebas estadísticas de uniformidad

Para la uniformidad:
$$
H_0:R_i \thicksim U(0, 1) \\
H_1:R_i \nsim U(0, 1)
$$

### Prueba Kolmogorov-Smirnof
Se basa en la distribución de probabilidad acumulada de la distribución uniforme ideal, y la compara con la distribución de probabilidad acumulada empírica (proviene de los datos).

El procedimiento es:

1. Ordenamos los datos.
2. 

**Ejemplo:**
```{r}
rgenerator_data <- c(0.05, 0.14, 0.44, 0.81, 0.93)
f_x <- c(0, 0.2, 0.4, 0.6, 0.8) # Aquí al final va el 1.

d1 = f_x - rgenerator_data
d2 <- c(0.15, 0.26, 0.16, 0.01, 0.07)
```

El mayor valor de diferencia es 0.26, y el valor crítico para la prueba con $\alpha = 0.05$ es $D_critico = 0.565$; por lo tanto, no se puede rechazar la hipótesis nula.

### Prueba Chi-cuadrado

> Los grados de libertad son la cantidad de intervalos que hay menos 1.

## Pruebas estadísticas de independencia
Se aplica un proceso de autocorrelación con los números que fueron generados.

**Ejemplo:** Sea la secuencia de datos:
```{r}
numbers_seq <- c(0.12, 0.01, 0.23, 0.28, 0.89, 0.31, 0.64, 0.28, 0.83, 0.93, 0.99, 0.15, 0.33, 0.35, 0.91, 0.41, 0.60, 0.27, 0.75, 0.88, 0.68, 0.49, 0.05, 0.43, 0.95, 0.58, 0.19, 0.36, 0.69, 0.87)

M = 4
lag = 5
# Seleccionar de 5 al otro 5 y esos utilizarlos para hacer el estimador.
candidates <- numbers_seq[seq()]
estimator_p = (1 / M + 1) * sum()
```

# Simulación Montecarlo

## Integración Montecarlo

Es un método que nos permite calcular áreas utilizando varibles estocásticas.

Los pasos para el algoritmo son:

1. Se generan varios números aleatorios uniformemente distribuidos para cada una de las dimensiones.
2. Se evalúa la función en cada uno de los números generados

**Ejemplo:** Estimación de PI.
```{r}
x_values = runif(10000)
y_values = runif(10000)

inCircle = (x_values ** 2 + y_values ** 2) <= 1

points_circle = cbind(x_values[inCircle], y_values[inCircle])

pi_aproximation = 4 * length(points_circle[, 1]) / length(x_values)
```

**Ejercicio:** Estimar el valor de la siguiente integral.
$$
\int^2_{-2} e^{x + x^2}dx
$$
Usando la sustitución.
$$
y = \frac{x + 2}{4} \rightarrow dy = \frac{1}{4} dx \\
x = 4y - 2,\ dx = 4dy
$$
Construimos la nueva integral.
$$
\int^1_0 e^{4y - 2 + 8y^2 - 16y + 4} 4dy \rightarrow \int^1_0 4e^{2 + 12y + 16y^2} dy \\
4e^2 \int^1_0 e^{4(3y + 4y^2)} dy
$$

Estimamos el valor de la integral.
```{r}
x_values = runif(100)
y_values = runif(100)

funIntegral <- function(domain_value) {
  res = exp(4 * (3 * domain_value + 4 * (domain_value**2)))
  return(res)
}

underCurve = matrix(data = c(0, 0), ncol = 2)

for(iter in 1:length(x_values)) {
  flag = funIntegral(x_values[iter]) >= y_values[iter]
  if (flag) {
    underCurve = rbind(underCurve, c(x_values[iter], y_values[iter]))
  }
}

area_aproximation = (length(underCurve[, 1]) - 1) / length(x_values)
```

```{r}
curve(expr = (exp(4 * (3 * x + 4 * (x**2)))), from = 0, to = 1)
points(x = underCurve[, 1], y = underCurve[, 2])
```

# Generación de variables aleatorias discretas

## Distribución Geométrica
Recordando la función masa de probabilidad:
$$
P(X = i) = pq^{i - 1}, 
$$

```{r}
plot(0:10, dgeom(0:10, prob = 0.8), type = "h", col = "red")
```

```{r}
random_geometric <- function(trials, prob_success) {
  prob_fail = 1 - prob_success
  
  res = c()
  for (k in 1:trials) {
    res = c(res, trunc(log10(runif(1)) / log10(prob_fail)) + 1)
  }
  
  return(res)
}
```

```{r}
plot(1:1000, random_geometric(1000, 0.8))
```

```{r}
table(random_geometric(1000, 0.8))
hist(random_geometric(1000, 0.8), freq = FALSE, breaks = 5)
```

## Distribución de Poisson

Utilizando la misma técnica de la transformada inversa podemos generar valores aleatorios con distribución de Poisson. Recordamos la función masa:
$$
P()\\
FALTA
$$

```{r}
random_poisson <- function(trials, lambda) {
  
  res = c()
  for(trial in 1:trials) {
    limit = runif(n = 1)
    
    i = 0
    p = exp(-lambda)
    F = p
    
    while(limit > F) {
      p = (lambda * p) / (i + 1)
      F = F + p
      i = i + 1
    }
    res = c(res, i)
  }
  return(res)
}
```

```{r}
table(random_poisson(100, 10))
```


```{r}
hist(random_poisson(100, 10), breaks = 12)
```

# Generación de variables aleatorias continuas

## Distribución Exponencial
```{r}
curve(dexp(x, rate = 1), from = 0, to = 10, main = "Distribución Exponencial", ylab = "Función de densidad")
```

Función inversa:
$$
x = -\frac{1}{\lambda}Ln(1 - F_{(x)})
$$
```{r}
inverse_exponential <- function(lambda, value) {
  value_inverse = - (1 / lambda) * log(value)
  return(value_inverse)
}

random_exponential <- function(trials, lambda) {
  res = c()
  
  for (trial in 1:trials) {
    res = c(res, inverse_exponential(lambda, runif(1)))
  }
  
  return(res)
}
```

```{r}
hist(random_exponential(1000, 1), main = "Histograma de valores aleatorios de una distribución exponencial")
```

**Ejercicio:** El periodo de vida en años de un televisor sigue una distribución exponencial con un promedio de vida de 6 años.

+ Suponga que se analizan 5 televisores, ¿cuál es la probabilidad de que al menos 3 de los 5 televisores fallen antes del segundo año de uso?
```{r}
tv_lambda = 1/6

tests = 10000
tv_fails_set = 0
for(tv_set in 1:tests) {
  sample_size = 5
  life_times = random_exponential(sample_size, tv_lambda)
  if(sum(life_times < 2) >= 3) {
    tv_fails_set = tv_fails_set + 1
  }
}

tv_fails_probability = tv_fails_set / tests

print(paste("Probabilidad:", tv_fails_probability))
```

La probabilidad de que al menos 3 de los televisores fallen antes del segundo año es `r tv_fails_probability`.

**Ejercicio:** Sea la siguiente función de probabilidad acumulada:
$$
F_{(x)} = x^k, 0 < x < 1
$$

*Realice una función que genere valores que sigan la distribución de la función de probabilidad*
Empezamos calculando la inversa de la función:
$$
(F_{(x)})^{\frac{1}{k}} = (x^k)^{\frac{1}{k}} \\
x = (F_{(x)})^{\frac{1}{k}}
$$

Con la función inversa, utilizamos el método de la transformada inversa, entonces asumimos que $F_{(x)} \sim U(0, 1)$. Creamos la función que genera los valores:
```{r}
inverse_excercise_function <- function(parameter_k, value) {
  x_value = value ** (1 / parameter_k)
  
  return(x_value)
}

random_excercise_generator <- function(trials, k) {
  
  random_values = c()
  for (trial in 1:trials) {
    random_values = c(random_values, inverse_excercise_function(parameter_k = k, value = runif(1)))
  }
  
  return(random_values)
}
```

Ahora debemos calcular la función de densidad para poder comprobar visualmente que nuestra función de generación aleatoria funciona correctamente:
$$
f_{(x)} = \frac{dF_{(x)}}{dx} \\
f_{(x)} = kx^{k - 1}
$$

Construimos la gráfica de la distribución de probabilidad en conjunto con la aproximación que se genera con la función de generación.
```{r}
hist(random_excercise_generator(1000, 10), freq = FALSE, breaks = 15)
curve(10 * (x ** (9)), from = 0, to = 1, col = 'blue', add = TRUE)
```

Como vemos, las densidades formadas por la función de generación aleatoria si se ajustan a la función de densidad de probabilidad.

